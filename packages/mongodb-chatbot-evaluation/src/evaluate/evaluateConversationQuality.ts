import { ConversationGeneratedData } from "../generate/GeneratedDataStore";
import { EvaluateQualityFunc } from "./EvaluateQualityFunc";
import { strict as assert } from "assert";
import {
  ResponseQualityExample,
  checkResponseQuality,
} from "./checkResponseQuality";
import { ObjectId } from "mongodb-rag-core/mongodb";
import { OpenAI } from "mongodb-rag-core/openai";
import { EvalResult } from "./EvaluationStore";
import { stringifyConversation } from "./stringifyConversation";

export interface EvaluateConversationQualityParams {
  openAiClient: OpenAI;

  /**
    The name of the OpenAI ChatGPT API deployment to use.
    @example "gpt-3.5-turbo"
   */
  deploymentName: string;

  /**
    Provide a few examples of conversation transcripts, expected outputs,
    and what the LLM output should be.
    This is _extremely_ useful for helping the LLM understand
    its expected behavior for your use case.
    While not strictly necessary, it is highly recommended
    to include a few representative examples.

    Here, the LLM utilizes a prompting technique called ["few-shot prompting"](https://www.promptingguide.ai/techniques/fewshot.en).
   */
  fewShotExamples?: ResponseQualityExample[];
}

/**
  Construct a a {@link EvaluateQualityFunc} that evaluates the quality of a conversation
  using an OpenAI ChatGPT LLM.

  The returned {@link EvalResult} has the following properties:

  - In {@link EvalResult.result}, `1` if the conversation meets quality standards and `0` if it does not.
  - In {@link EvalResult.metadata}, `reason` for the result, as generated by the LLM.
 */
export function makeEvaluateConversationQuality({
  openAiClient,
  deploymentName,
}: EvaluateConversationQualityParams): EvaluateQualityFunc {
  return async ({ runId, generatedData }) => {
    assert(
      generatedData.type === "conversation",
      "Invalid data type. Expected 'conversation' data."
    );
    const conversationData = generatedData as ConversationGeneratedData;
    const {
      data: { messages },
    } = conversationData;
    const conversationTranscript = stringifyConversation(messages);
    const { qualitativeFinalAssistantMessageExpectation, name } =
      conversationData.evalData;
    assert(
      qualitativeFinalAssistantMessageExpectation,
      "No expectation provided"
    );
    const { meetsChatQualityStandards, reason } = await checkResponseQuality({
      deploymentName,
      openAiClient,
      expectedOutputDescription: qualitativeFinalAssistantMessageExpectation,
      received: conversationTranscript,
    });
    const result = {
      _id: new ObjectId(),
      generatedDataId: generatedData._id,
      commandRunMetadataId: runId,
      type: "conversation_quality",
      result: meetsChatQualityStandards ? 1 : 0,
      createdAt: new Date(),
      metadata: {
        testName: name,
        reason,
        conversationTranscript,
        qualitativeFinalAssistantMessageExpectation,
      },
    } satisfies EvalResult;
    return result;
  };
}
