import { strict as assert } from "assert";
import {
  Request as ExpressRequest,
  Response as ExpressResponse,
} from "express";
import { DbMessage } from "mongodb-rag-core";
import { ObjectId } from "mongodb-rag-core/mongodb";
import {
  ConversationsService,
  Conversation,
  SomeMessage,
  makeDataStreamer,
} from "mongodb-rag-core";
import { ApiMessage, RequestError, makeRequestError } from "./utils";
import { getRequestId, sendErrorResponse } from "../../utils";
import { z } from "zod";
import { SomeExpressRequest } from "../../middleware/validateRequestSchema";
import {
  AddCustomDataFunc,
  ConversationsRouterLocals,
} from "./conversationsRouter";
import { Logger } from "mongodb-rag-core/braintrust";
import { UpdateTraceFunc, updateTraceIfExists } from "./UpdateTraceFunc";
import { GenerateResponse } from "../../processors/GenerateResponse";
import { OpenAI } from "mongodb-rag-core/openai";

export const DEFAULT_MAX_INPUT_LENGTH = 3000; // magic number for max input size for LLM
export const DEFAULT_MAX_USER_MESSAGES_IN_CONVERSATION = 7; // magic number for max messages in a conversation

export type CreateResponseRequestBody = z.infer<
  typeof CreateResponseRequestBodySchema
>;

const MessageStatusSchema = z
  .enum(["in_progress", "completed", "incomplete"])
  .optional()
  .describe(
    "The status of the item. One of `in_progress`, `completed`, or `incomplete`. Populated when items are returned via API."
  );

export const CreateResponseRequestBodySchema = z.object({
  model: z.string(),
  instructions: z.string().optional(),
  input: z.union([
    z.string(),

    z.array(
      z.union([
        z.object({
          role: z.enum(["user", "assistant", "system"]),
          content: z.string(),
          type: z.literal("message").optional(),
        }),
        // function tool call
        z.object({
          arguments: z
            .string()
            .describe("JSON string of arguments passed to the function"),
          name: z.string().describe("Name of the function to run"),
          type: z.literal("function_call"),
          id: z.string().optional().describe("Unique ID of the function call"),
          status: MessageStatusSchema,
        }),
        // function tool call output
        z.object({
          call_id: z
            .string()
            .describe(
              "Unique ID of the function tool call generated by the model"
            ),
          output: z.string().describe("JSON string of the function tool call"),
          type: z.literal("function_call_output"),
          id: z
            .string()
            .optional()
            .describe(
              "The unique ID of the function tool call output. Populated when this item is returned via API."
            ),
          status: MessageStatusSchema,
        }),
      ])
    ),
  ]),
  max_output_tokens: z.number().max(4000).optional().default(1000),
  metadata: z
    .record(z.string(), z.string().max(512))
    .optional()
    .refine(
      (metadata) => Object.keys(metadata ?? {}).length <= 16,
      "Too many metadata fields. Max 16."
    ),
  previous_response_id: z
    .string()
    .optional()
    .describe(
      "The unique ID of the previous response to the model. Use this to create multi-turn conversations."
    ),
  store: z
    .boolean()
    .optional()
    .describe("Whether to store the response in the conversation.")
    .default(true),
  stream: z.literal(true, {
    errorMap: () => ({ message: "'stream' must be true" }),
  }),
  temperature: z
    .union([
      z.literal(0, {
        errorMap: () => ({ message: "Temperature must be 0 or unset" }),
      }),
      z.undefined(),
    ])
    .optional()
    .describe("Temperature for the model. Defaults to 0.")
    .default(0),
  tool_choice: z
    .union([
      z.enum(["none", "only", "auto"]),
      z
        .object({
          name: z.string(),
          type: z.literal("function"),
        })
        .describe("Function tool choice"),
    ])
    .optional()
    .describe("Tool choice for the model. Defaults to 'auto'.")
    .default("auto"),
  tools: z
    .array(
      z.object({
        name: z.string(),
        description: z.string().optional(),
        parameters: z
          .record(z.string(), z.unknown())
          .describe(
            "A JSON schema object describing the parameters of the function."
          ),
      })
    )
    .optional()
    .describe(
      "Tools for the model to use. Required if tool_choice is 'function'."
    ),

  user: z.string().optional().describe("The user ID of the user."),
});

export const CreateResponseRequest = SomeExpressRequest.merge(
  z.object({
    headers: z.object({
      "req-id": z.string(),
    }),
    body: CreateResponseRequestBodySchema,
  })
);

export type CreateResponseRequest = z.infer<typeof CreateResponseRequest>;

export interface CreateResponseRouteParams {
  conversations: ConversationsService;
  maxInputLengthCharacters?: number;
  maxUserMessagesInConversation?: number;
  generateResponse: GenerateResponse;
  addMessageToConversationCustomData?: AddCustomDataFunc;
  openAi: OpenAI;
  /**
    If present, the route will create a new conversation
    when given the `conversationIdPathParam` in the URL.
   */
  createConversation?: {
    /**
      Create a new conversation when the `conversationId` is the string "null".
     */
    createOnNullConversationId: boolean;
    /**
      The custom data to add to the new conversation
      when it is created.
     */
    addCustomData?: AddCustomDataFunc;
  };

  /**
    Custom function to update the Braintrust tracing
    after the response has been sent to the user.
    Can add additional tags, scores, etc.
   */
  updateTrace?: UpdateTraceFunc;
  braintrustLogger?: Logger<true>;
  supportedModels: string[];
}

export function makeCreateResponseRoute({
  conversations,
  generateResponse,
  maxInputLengthCharacters = DEFAULT_MAX_INPUT_LENGTH,
  maxUserMessagesInConversation = DEFAULT_MAX_USER_MESSAGES_IN_CONVERSATION,
  addMessageToConversationCustomData,
  supportedModels,
  updateTrace,
}: CreateResponseRouteParams) {
  return async (
    req: ExpressRequest<
      CreateResponseRequest["params"],
      unknown,
      CreateResponseRequest["body"]
    >,
    res: ExpressResponse<ApiMessage, ConversationsRouterLocals>
  ) => {
    const dataStreamer = makeDataStreamer();
    const reqId = getRequestId(req); // TODO: figure this one out...
    try {
      const {
        body: {
          input,
          model,
          metadata,
          previous_response_id,
          store,
          stream,
          temperature,
          tool_choice,
          tools,
          user,
          max_output_tokens,
          instructions,
        },
        ip,
      } = req;

      // --- MODEL CHECK ---
      if (!supportedModels.includes(model)) {
        throw makeRequestError({
          httpStatus: 400,
          message: `Model ${model} is not supported.`,
        });
      }

      // --- MAX INPUT LENGTH CHECK ---
      if (JSON.stringify(input).length > maxInputLengthCharacters) {
        throw makeRequestError({
          httpStatus: 400,
          message: "Message too long",
        });
      }

      const customData = await getCustomData({
        req,
        res,
        addMessageToConversationCustomData,
      });

      // --- LOAD CONVERSATION ---
      const conversation = await loadConversationByMessageId({
        messageId: previous_response_id,
        conversations,
      });

      // --- MAX CONVERSATION LENGTH CHECK ---
      // TODO: both these checks same as in addMessageToConversation, make DRY
      const numUserMessages = conversation.messages.reduce(
        (acc, message) => (message.role === "user" ? acc + 1 : acc),
        0
      );
      if (numUserMessages >= maxUserMessagesInConversation) {
        // Omit the system prompt and assume the user always received one response per message
        throw makeRequestError({
          httpStatus: 400,
          message: `Too many messages. You cannot send more than ${maxUserMessagesInConversation} messages in this conversation.`,
        });
      }

      if (stream !== true) {
        throw makeRequestError({
          httpStatus: 400,
          message: "Stream must be true",
        });
      }

      if (stream) {
        dataStreamer.connect(res);
      }

      const assistantResponseMessageId = new ObjectId();

      const streamingMetadata = {
        conversationId: conversation._id.toString(),
      };
      const baseResponse = {
        id: assistantResponseMessageId.toHexString(),
        model: model,
        object: "response" as const,
        created_at: Date.now(),
        temperature: temperature,
        instructions: instructions ?? null,
        metadata: {
          // ...Any other stuff that should be in here? something for verified answers?
          ...streamingMetadata,
        },
        parallel_tool_calls: false,
        tool_choice: tool_choice === "only" ? "auto" : tool_choice,
        tools:
          tools?.map((tool) => {
            return {
              name: tool.name,
              description: tool.description,
              parameters: tool.parameters,
              container: null,
              strict: null,
              type: "function",
            };
          }) ?? [],
        top_p: null,
        // TODO: below here, i think these all need additional work...
        // TODO: 2x check if the following should be different if incomplete
        incomplete_details: null,
        // TODO: 2x check if the following should be different if incomplete
        error: null,
        // TODO: output
        output: [],
        output_text: "",
      } satisfies OpenAI.Responses.ResponseCreatedEvent["response"];

      dataStreamer.streamResponsesApiPart({
        type: "response.created",
        // TODO: fix typescript stuff
        response: {
          ...baseResponse,
        },
      } satisfies Omit<OpenAI.Responses.ResponseCreatedEvent, "sequence_number">);

      // TODO: make a Responses API version of generate response
      const { messages } = await generateResponse({
        latestMessageText,
        clientContext,
        customData,
        dataStreamer,
        shouldStream,
        reqId,
        conversation,
        traceId,
      });

      // --- SAVE QUESTION & RESPONSE ---
      const dbNewMessages = await addMessagesToDatabase({
        conversations,
        conversation,
        messages,
        assistantResponseMessageId,
      });
      const dbAssistantMessage = dbNewMessages[dbNewMessages.length - 1];

      assert(dbAssistantMessage !== undefined, "No assistant message found");

      dataStreamer.streamResponsesApiPart({
        type: "response.completed",
        // TODO: better figure out typescripting here...
        response: {
          ...baseResponse,
          output_text: dbAssistantMessage.content,
          output: dbMessageToResponseOutputItem(dbNewMessages),
        },
      } satisfies Omit<OpenAI.Responses.ResponseCompletedEvent, "sequence_number">);
      if (dataStreamer.connected) {
        dataStreamer.disconnect();
      }

      await updateTraceIfExists({
        updateTrace,
        reqId,
        conversations,
        conversationId: conversation._id,
        assistantResponseMessageId: dbAssistantMessage.id,
      });
    } catch (error) {
      // TODO: better error handling, in line with the Responses API
      const { httpStatus, message } =
        (error as Error).name === "RequestError"
          ? (error as RequestError)
          : makeRequestError({
              message: (error as Error).message,
              stack: (error as Error).stack,
              httpStatus: 500,
            });

      sendErrorResponse({
        res,
        reqId,
        httpStatus,
        errorMessage: message,
      });
    } finally {
      if (dataStreamer.connected) {
        dataStreamer.disconnect();
      }
    }
  };
}

// --- HELPERS ---

// TODO: implement...
function dbMessageToResponseOutputItem(
  dbMessages: DbMessage<SomeMessage>[]
): OpenAI.Responses.ResponseOutputItem[] {
  return [];
}

// TODO: this is same as in addMessageToConversation
// ...have separate helper imported by both
async function getCustomData({
  req,
  res,
  addMessageToConversationCustomData,
}: {
  req: ExpressRequest;
  res: ExpressResponse<ApiMessage, ConversationsRouterLocals>;
  addMessageToConversationCustomData?: AddCustomDataFunc;
}) {
  try {
    return addMessageToConversationCustomData
      ? await addMessageToConversationCustomData(req, res)
      : undefined;
  } catch (_err) {
    throw makeRequestError({
      httpStatus: 500,
      message: "Unable to process custom data",
    });
  }
}

// TODO: this is same as in addMessageToConversation
// ...have separate helper imported by both
interface AddMessagesToDatabaseParams {
  conversation: Conversation;
  conversations: ConversationsService;
  messages: SomeMessage[];
  assistantResponseMessageId: ObjectId;
  store: boolean;
}
async function addMessagesToDatabase({
  conversation,
  conversations,
  messages,
  assistantResponseMessageId,
  // TODO: handle if store is false, only include metadata, no content.
  store,
}: AddMessagesToDatabaseParams) {
  (
    messages as Parameters<
      typeof conversations.addManyConversationMessages
    >[0]["messages"]
  )[messages.length - 1].id = assistantResponseMessageId;

  const conversationId = conversation._id;
  const dbMessages = await conversations.addManyConversationMessages({
    conversationId,
    messages,
  });
  return dbMessages;
}

async function loadConversationByMessageId({
  messageId,
  conversations,
  customData,
}: {
  messageId?: string;
  conversations: ConversationsService;
  customData?: Record<string, unknown>;
}): Promise<Conversation> {
  if (!messageId) {
    return await conversations.create({
      customData,
    });
  }
  const conversation = await conversations.findByMessageId({
    messageId: ObjectId.createFromHexString(messageId),
  });
  if (!conversation) {
    throw makeRequestError({
      httpStatus: 404,
      message: `Message ${messageId} not found`,
    });
  }
  return conversation;
}
